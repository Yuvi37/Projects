{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72fd83e3",
   "metadata": {},
   "source": [
    "\n",
    "# Demand forecasting with the Temporal Fusion Transformer (Stallion tutorial)\n",
    "\n",
    "This notebook mirrors the PyTorch Forecasting tutorial that trains a **TemporalFusionTransformer (TFT)**\n",
    "on the small **Stallion** beverage sales dataset. The goal is to forecast **6 months** of `volume`\n",
    "per (agency, SKU) using time-varying and static features.\n",
    "\n",
    "We will:\n",
    "1. Load and enrich the dataset with a time index and engineered features.\n",
    "2. Build `TimeSeriesDataSet` objects and dataloaders.\n",
    "3. Establish a simple **Baseline**.\n",
    "4. Configure and train a **TFT** with PyTorch Lightning.\n",
    "5. (Optional) Explore **hyperparameter tuning** with Optuna.\n",
    "6. Evaluate on validation data and visualize predictions/interpretability outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5942278",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # keep logs clean\n",
    "\n",
    "import copy\n",
    "from pathlib import Path\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e88373",
   "metadata": {},
   "source": [
    "\n",
    "## Load data\n",
    "\n",
    "We use the built-in **Stallion** dataset helper and add useful features:\n",
    "\n",
    "- `time_idx` (monotonic index per time step),\n",
    "- calendar month (categorical),\n",
    "- log-volume,\n",
    "- cross-sectional rolling means by `sku` and by `agency`,\n",
    "- special days compressed into a single categorical via reversing one-hot columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1123b1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pytorch_forecasting.data.examples import get_stallion_data\n",
    "\n",
    "data = get_stallion_data()\n",
    "\n",
    "# add time index\n",
    "data[\"time_idx\"] = data[\"date\"].dt.year * 12 + data[\"date\"].dt.month\n",
    "data[\"time_idx\"] -= data[\"time_idx\"].min()\n",
    "\n",
    "# add additional features\n",
    "data[\"month\"] = data.date.dt.month.astype(str).astype(\"category\")  # categories must be strings\n",
    "data[\"log_volume\"] = np.log(data.volume + 1e-8)\n",
    "data[\"avg_volume_by_sku\"] = data.groupby([\"time_idx\", \"sku\"], observed=True).volume.transform(\"mean\")\n",
    "data[\"avg_volume_by_agency\"] = data.groupby([\"time_idx\", \"agency\"], observed=True).volume.transform(\"mean\")\n",
    "\n",
    "# compress special days into a single categorical (reverse one-hot)\n",
    "special_days = [\n",
    "    \"easter_day\",\n",
    "    \"good_friday\",\n",
    "    \"new_year\",\n",
    "    \"christmas\",\n",
    "    \"labor_day\",\n",
    "    \"independence_day\",\n",
    "    \"revolution_day_memorial\",\n",
    "    \"regional_games\",\n",
    "    \"fifa_u_17_world_cup\",\n",
    "    \"football_gold_cup\",\n",
    "    \"beer_capital\",\n",
    "    \"music_fest\",\n",
    "]\n",
    "data[special_days] = data[special_days].apply(lambda x: x.map({0: \"-\", 1: x.name})).astype(\"category\")\n",
    "\n",
    "data.sample(5, random_state=521)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd5b885",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# quick describe\n",
    "data.describe(include=\"all\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a86d35",
   "metadata": {},
   "source": [
    "\n",
    "## Create dataset and dataloaders\n",
    "\n",
    "We build a `TimeSeriesDataSet` describing inputs/targets and metadata such as which variables are\n",
    "static vs time-varying and known vs unknown. We also specify **group-wise normalization**.\n",
    "Validation consists of the last 6 months for each series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31d8925",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_prediction_length = 6\n",
    "max_encoder_length = 24\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"volume\",\n",
    "    group_ids=[\"agency\", \"sku\"],\n",
    "    min_encoder_length=max_encoder_length // 2,  # keep encoder fairly long\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"agency\", \"sku\"],\n",
    "    static_reals=[\"avg_population_2017\", \"avg_yearly_household_income_2017\"],\n",
    "    time_varying_known_categoricals=[\"special_days\", \"month\"],\n",
    "    variable_groups={\"special_days\": special_days},  # treat the special days as one categorical group\n",
    "    time_varying_known_reals=[\"time_idx\", \"price_regular\", \"discount_in_percent\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"volume\",\n",
    "        \"log_volume\",\n",
    "        \"industry_volume\",\n",
    "        \"soda_volume\",\n",
    "        \"avg_max_temp\",\n",
    "        \"avg_volume_by_agency\",\n",
    "        \"avg_volume_by_sku\",\n",
    "    ],\n",
    "    target_normalizer=GroupNormalizer(groups=[\"agency\", \"sku\"], transformation=\"softplus\"),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "# validation predicts last max_prediction_length points for each series\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "\n",
    "# create dataloaders\n",
    "batch_size = 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130b6884",
   "metadata": {},
   "source": [
    "\n",
    "## Baseline\n",
    "\n",
    "A naive baseline repeats the last observed value across the horizon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d737fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# baseline MAE\n",
    "baseline_predictions = Baseline().predict(val_dataloader, return_y=True)\n",
    "MAE()(baseline_predictions.output, baseline_predictions.y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f7500e",
   "metadata": {},
   "source": [
    "\n",
    "## Train the Temporal Fusion Transformer\n",
    "\n",
    "We'll set seeds, construct a small TFT model, and (optionally) use Lightning's LR finder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34132c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "trainer_lr = pl.Trainer(\n",
    "    accelerator=\"cpu\",\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "tft_lr = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,      # final choice (can adjust based on finder)\n",
    "    hidden_size=8,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    loss=QuantileLoss(),\n",
    "    optimizer=\"ranger\",\n",
    ")\n",
    "\n",
    "print(f\"Number of parameters in network (LR search setup): {tft_lr.size()/1e3:.1f}k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7dbf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: learning rate finder (comment out if undesired)\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "res = Tuner(trainer_lr).lr_find(\n",
    "    tft_lr,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "_ = res.plot(show=True, suggest=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6e0a4a",
   "metadata": {},
   "source": [
    "\n",
    "### Full training\n",
    "\n",
    "We now configure callbacks and train a slightly larger TFT.\n",
    "Use TensorBoard to monitor training: `tensorboard --logdir lightning_logs`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a0e2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()\n",
    "logger = TensorBoardLogger(\"lightning_logs\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"cpu\",\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=50,  # demo-friendly; increase for full training\n",
    "    callbacks=[lr_logger, early_stop],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=2,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,\n",
    "    optimizer=\"ranger\",\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "trainer.fit(tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eac8f64",
   "metadata": {},
   "source": [
    "\n",
    "## Hyperparameter tuning (Optuna) â€” optional\n",
    "\n",
    "`optimize_hyperparameters` can search across ranges for key TFT hyperparameters.\n",
    "This can be time-consuming; reduce `n_trials` or `max_epochs` for quick runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215b9ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# WARNING: This can take a long time. Adjust n_trials and max_epochs for your setup.\n",
    "# import pickle\n",
    "# study = optimize_hyperparameters(\n",
    "#     train_dataloader,\n",
    "#     val_dataloader,\n",
    "#     model_path=\"optuna_tft_study\",\n",
    "#     n_trials=20,\n",
    "#     max_epochs=50,\n",
    "#     gradient_clip_val_range=(0.01, 1.0),\n",
    "#     hidden_size_range=(8, 128),\n",
    "#     hidden_continuous_size_range=(8, 128),\n",
    "#     attention_head_size_range=(1, 4),\n",
    "#     learning_rate_range=(0.001, 0.1),\n",
    "#     dropout_range=(0.1, 0.3),\n",
    "#     trainer_kwargs=dict(limit_train_batches=30),\n",
    "#     reduce_on_plateau_patience=4,\n",
    "#     use_learning_rate_finder=False,\n",
    "# )\n",
    "# with open(\"optuna_tft_study.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(study, f)\n",
    "# print(study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090f018b",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluate performance & visualize\n",
    "\n",
    "We reload the best checkpoint, compute MAE on validation data, and visualize predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ce00a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load best model\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "\n",
    "# MAE on validation set\n",
    "predictions = best_tft.predict(val_dataloader, return_y=True, trainer_kwargs=dict(accelerator=\"cpu\"))\n",
    "MAE()(predictions.output, predictions.y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e605ce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot a few validation predictions with attention\n",
    "raw = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True, trainer_kwargs=dict(accelerator=\"cpu\"))\n",
    "for idx in range(3):  # fewer plots for convenience\n",
    "    best_tft.plot_prediction(raw.x, raw.output, idx=idx, add_loss_to_title=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b00eecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# worst-performers by SMAPE\n",
    "predictions = best_tft.predict(val_dataloader, return_y=True, trainer_kwargs=dict(accelerator=\"cpu\"))\n",
    "mean_losses = SMAPE(reduction=\"none\").loss(predictions.output, predictions.y[0]).mean(1)\n",
    "indices = mean_losses.argsort(descending=True)\n",
    "for idx in range(3):\n",
    "    best_tft.plot_prediction(raw.x, raw.output, idx=indices[idx], add_loss_to_title=SMAPE(quantiles=best_tft.loss.quantiles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5145a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Actuals vs predictions across variable bins\n",
    "pred_x = best_tft.predict(val_dataloader, return_x=True, trainer_kwargs=dict(accelerator=\"cpu\"))\n",
    "pv = best_tft.calculate_prediction_actual_by_variable(pred_x.x, pred_x.output)\n",
    "best_tft.plot_prediction_actual_by_variable(pv);\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
